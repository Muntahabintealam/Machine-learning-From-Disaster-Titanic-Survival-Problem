{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"},{"sourceId":9448647,"sourceType":"datasetVersion","datasetId":5552551}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Survival Prediction with Random Forest Classifier\nThis project uses the Titanic dataset to predict survival using machine learning models. The model is built using a `RandomForestClassifier` from Scikit-learn. The following steps outline the data preprocessing, feature engineering, model training, and evaluation.\n##  Import Libraries\nFirst,lets  import the necessary Python libraries for data manipulation, model building, and evaluation.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-01T20:46:46.853261Z","iopub.execute_input":"2024-10-01T20:46:46.853728Z","iopub.status.idle":"2024-10-01T20:46:46.860097Z","shell.execute_reply.started":"2024-10-01T20:46:46.853685Z","shell.execute_reply":"2024-10-01T20:46:46.858848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Load the Dataset\nlets load the Titanic dataset into pandas DataFrames for training and testing.","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndata_file_path :str = '/kaggle/input/titanic/train.csv'\ntrain_data = pd.read_csv(data_file_path)\ntest_file_path :str = '/kaggle/input/titanic/test.csv'\ntest_data = pd.read_csv(test_file_path)\ntrain_data.head(20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Data Preprocessing Pipeline and Feature Engineering\nThe following steps outline how we handle missing values, create new features, and prepare the data for model training.\n\n\n\nWe handle missing data in the `Age`, `Embarked`, and `Cabin` columns. \n\n- **Age:** Missing values are filled with the median value of the `Age` column.\n- **Embarked:** Missing values are filled with the most frequent value (mode) of the `Embarked` column.\n- **Cabin:** This column contains too many missing values, so it is dropped from the dataset.\n\nA new feature FamilySize is created by summing the SibSp (siblings/spouses aboard) and Parch (parents/children aboard) columns. This feature represents the total number of family members a passenger has aboard the Titanic.\n\nCategorical columns such as Sex and Embarked are converted into numerical values using one-hot encoding. This allows machine learning algorithms to process the categorical data.\n","metadata":{}},{"cell_type":"code","source":"# Handling missing values\ntrain_data['Age'].fillna(train_data['Age'].median(), inplace=True)\ntrain_data['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace=True)\ntrain_data.drop(columns=['Cabin'], inplace=True)\n\n# Create a new feature 'FamilySize'\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch']\n\n# Convert categorical columns to numerical using one-hot encoding\ntrain_data = pd.get_dummies(train_data, columns=['Sex', 'Embarked'], drop_first=True)\n\n# Drop unnecessary columns\ntrain_data_cleaned = train_data.drop(columns=['Name', 'Ticket', 'PassengerId'])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T20:46:46.918952Z","iopub.execute_input":"2024-10-01T20:46:46.919478Z","iopub.status.idle":"2024-10-01T20:46:46.954242Z","shell.execute_reply.started":"2024-10-01T20:46:46.919424Z","shell.execute_reply":"2024-10-01T20:46:46.953023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After cleaning the data, we define the feature matrix (X) and the target variable (y). The target variable y is the Survived column, while all other columns form the feature matrix X.\n\nThe dataset is split into training and validation sets using an 80/20 ratio.\n","metadata":{}},{"cell_type":"code","source":"# Define feature matrix (X) and target vector (y)\nX = train_data_cleaned.drop(columns=['Survived'])\ny = train_data_cleaned['Survived']\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training and Evaluation: Random Forest Classifier\n\nIn this section, we train a **Random Forest Classifier** on the training dataset and evaluate its performance on the validation set. We also compute common evaluation metrics such as accuracy, confusion matrix, and classification report.\n\n---\n\n### 1. Training the Random Forest Classifier\n\nWe initialize a `RandomForestClassifier` with a fixed random state.\n\n### 2. Prediction and Evaluation\nOnce the model is trained, we use it to predict the target values (y_pred) for the validation set (X_val).\nWe evaluate the model using the following metrics:\n\nAccuracy: Measures the proportion of correct predictions.\nConfusion Matrix: Provides a summary of prediction results and shows the counts of true positives, true negatives, false positives, and false negatives.\nClassification Report: Gives precision, recall, and F1-score for each class.","metadata":{}},{"cell_type":"code","source":"# Train a Random Forest Classifier\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict on validation set\ny_pred = model.predict(X_val)\n\n# Evaluate the model\naccuracy = accuracy_score(y_val, y_pred)\nconf_matrix = confusion_matrix(y_val, y_pred)\nclass_report = classification_report(y_val, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\nprint(f\"Classification Report:\\n{class_report}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T20:46:46.955634Z","iopub.execute_input":"2024-10-01T20:46:46.956035Z","iopub.status.idle":"2024-10-01T20:46:47.277671Z","shell.execute_reply.started":"2024-10-01T20:46:46.955972Z","shell.execute_reply":"2024-10-01T20:46:47.276418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance\nVisualize the importance of each feature in determining survival using the trained Random Forest model.","metadata":{}},{"cell_type":"code","source":"\n\n# Get feature importance\nfeature_importance = model.feature_importances_\n\n# Create a DataFrame for visualization\nfeatures_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importance\n}).sort_values(by='Importance', ascending=False)\n\n# Plot feature importance\nplt.figure(figsize=(10,6))\nsns.barplot(x='Importance', y='Feature', data=features_df)\nplt.title('Feature Importance')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T20:46:47.280683Z","iopub.execute_input":"2024-10-01T20:46:47.281329Z","iopub.status.idle":"2024-10-01T20:46:47.658214Z","shell.execute_reply.started":"2024-10-01T20:46:47.281269Z","shell.execute_reply":"2024-10-01T20:46:47.657161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying Preprocessing and Making Predictions on Test Data\n\nIn this section, we apply the same preprocessing steps used for the training data to the test dataset. After preparing the data, we make predictions using the trained Random Forest model and generate a submission file in CSV format.\n\n\n","metadata":{}},{"cell_type":"code","source":"\n\n# Apply preprocessing steps\ntest_data['Age'].fillna(train_data['Age'].median(), inplace=True)\ntest_data['Fare'].fillna(train_data['Fare'].median(), inplace=True)\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch']\ntest_data = pd.get_dummies(test_data, columns=['Sex', 'Embarked'], drop_first=True)\n\n# Ensure the test data has the same columns as the training set\nmissing_cols = set(X.columns) - set(test_data.columns)\nfor col in missing_cols:\n    test_data[col] = 0\n\n# Make predictions\ntest_predictions = model.predict(test_data[X.columns])\n\n# Create a submission DataFrame\nsubmission = pd.DataFrame({\n    'PassengerId': test_data['PassengerId'],\n    'Survived': test_predictions\n})\n\n# Save the submission file\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T20:46:47.659657Z","iopub.execute_input":"2024-10-01T20:46:47.660063Z","iopub.status.idle":"2024-10-01T20:46:47.700072Z","shell.execute_reply.started":"2024-10-01T20:46:47.660023Z","shell.execute_reply":"2024-10-01T20:46:47.698956Z"},"trusted":true},"execution_count":null,"outputs":[]}]}